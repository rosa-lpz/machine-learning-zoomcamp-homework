{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0683b0cf-2e33-4b73-8cee-1ede5313f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_table(y_val, y_pred):\n",
    "    # np.linspace(start, stop, num) creates 101 thresholds between 0.0 and 1.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    \n",
    "    # List to store the results for each threshold\n",
    "    results = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        # Convert predicted probabilities to binary predictions using threshold t\n",
    "        y_pred = (y_pred >= t).astype(int)\n",
    "\n",
    "    \n",
    "        # Calculate precision and recall using sklearn metrics\n",
    "        p = precision_score(y_val, y_pred >= t)\n",
    "        r = recall_score(y_val, y_pred >= t)\n",
    "        \n",
    "        # Calculate true positives (TP), true negatives (TN), false positives (FP), false negatives (FN)\n",
    "        tp = np.sum((y_pred == 1) & (y_val == 1))\n",
    "        tn = np.sum((y_pred == 0) & (y_val == 0))\n",
    "        fp = np.sum((y_pred == 1) & (y_val == 0))\n",
    "        fn = np.sum((y_pred == 0) & (y_val == 1))\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred >= t)\n",
    "        # Store results for each threshold\n",
    "        results.append((t,tp, fp, fn, tn, p, r, accuracy))\n",
    "\n",
    "    # Create DataFrame with scores\n",
    "    columns = ['threshold','tp', 'fp', 'fn', 'tn', 'precision', 'recall','accuracy']\n",
    "    df_results= pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    # Add True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "    #df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)  # Sensitivity or Recall\n",
    "    #df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)  # Fall-out\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a0a1ec-f663-4dac-983d-516ff18b7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_table(y_val,y_pred):\n",
    "                # np.linspace(start, stop, num)\n",
    "    thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    scores=[]\n",
    "    #precision_recall= {}\n",
    "            \n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "        \n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "    \n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "    \n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0  # Avoid division by zero\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred >= t)\n",
    "        scores.append((t, tp, fp, fn, tn, p, r, accuracy))\n",
    "        \n",
    "    # Create DataFrame with scores\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn', 'p', 'r','accuracy']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    # Add True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)  # Sensitivity or Recall\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)  # Fall-out\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f49ae8-dbb8-464f-b643-e78d42f61520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_viz(y_val,y_pred):\n",
    "                # np.linspace(start, stop, num)\n",
    "           \n",
    "    thresholds = np.linspace(0.0, 1.0, 101)\n",
    "    scores=[]\n",
    "    #precision_recall= {}\n",
    "            \n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "        \n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "    \n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "    \n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0  # Avoid division by zero\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred >= t)\n",
    "        scores.append((t, tp, fp, fn, tn, p, r, accuracy))\n",
    "        \n",
    "    # Create DataFrame with scores\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn', 'p', 'r','accuracy']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    # Add True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)  # Sensitivity or Recall\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)  # Fall-out\n",
    "    \n",
    "    plt.plot(df_scores['threshold'], df_scores['p'], label='Precision')\n",
    "    plt.plot(df_scores['threshold'], df_scores['r'], label='Recall')\n",
    "    plt.plot(df_scores['threshold'], df_scores['accuracy'], label='Accuracy')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Precision and Recall vs Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d24b69f-4f63-449a-aa44-45885746e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_r_f1_table(y_val, y_pred):\n",
    "    # np.linspace(start, stop, num) creates 101 thresholds between 0.0 and 1.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 101)\n",
    "\n",
    "    scores=[]\n",
    "            \n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "        \n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "    \n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "    \n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0  # Avoid division by zero\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(y_val, y_pred >= t)\n",
    "\n",
    "        # Calculate F1 Score\n",
    "        f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "        # Store results for each threshold\n",
    "        scores.append((t, tp, fp, fn, tn, p, r,accuracy,f1))\n",
    "\n",
    "    # Create DataFrame with scores\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn', 'precision', 'recall','accuracy', 'F1']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    # Add True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)  # Sensitivity or Recall\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)  # Fall-out\n",
    "\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6973b90f-b847-4741-ad8f-a5d9b4f1c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_r_f1_viz(y_val, y_pred):\n",
    "    # np.linspace(start, stop, num) creates 101 thresholds between 0.0 and 1.0\n",
    "    thresholds = np.linspace(0.0, 1.0, 101)\n",
    "\n",
    "    scores=[]\n",
    "            \n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "        \n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "    \n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "    \n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0  # Avoid division by zero\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n",
    "\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) \n",
    "\n",
    "        # Calculate F1 Score\n",
    "        f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "        # Store results for each threshold\n",
    "        scores.append((t, tp, fp, fn, tn, p, r, accuracy, f1))\n",
    "\n",
    "    # Create DataFrame with scores\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn', 'precision', 'recall','accuracy', 'F1']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    #plt.plot(df_scores['threshold'], df_scores['p'], label='Precision')\n",
    "    #plt.plot(df_scores['threshold'], df_scores['r'], label='Recall')\n",
    "    #plt.plot(df_scores['threshold'], df_scores['accuracy'], label='Accuracy')\n",
    "    plt.plot(df_scores['threshold'], df_scores['F1'], label='F1')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('F1')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4c60c-05c4-4a83-b490-10fdc614dfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
